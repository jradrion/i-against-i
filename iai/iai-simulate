#!/usr/bin/env python

from iai.imports import *
from iai.helpers import *
from iai.simulator import *


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-d','--projectDir',dest='outDir',help='Directory for all project output. NOTE: the same projectDir must be used for all functions of i-against-i',default=None)
    parser.add_argument('-u_lo','--priorMu_lo',dest='mu_lo',help='Lower bound of prior on per-base mutation rate',type=float,default=1e-8)
    parser.add_argument('-u_hi','--priorMu_hi',dest='mu_hi',help='Upper bound of prior on per-base mutation rate',type=float,default=1e-8)
    parser.add_argument('-r_lo','--priorR_lo',dest='r_lo',help='Lower bound of prior on per-base recombination rate',type=float,default=1e-8)
    parser.add_argument('-r_hi','--priorR_hi',dest='r_hi',help='Upper bound of prior on per-base recombination rate',type=float,default=1e-8)
    parser.add_argument('-gr_lo','--priorGr_lo',dest='gr_lo',help='Lower bound of prior on population size growth rate',type=float,default=1e-6)
    parser.add_argument('-gr_hi','--priorGr_hi',dest='gr_hi',help='Upper bound of prior on population size growth rate',type=float,default=1e-6)
    parser.add_argument('-m_lo','--priorM_lo',dest='m_lo',help='Lower bound of prior on migration rate for admixture model',type=float,default=1e-7)
    parser.add_argument('-m_hi','--priorM_hi',dest='m_hi',help='Upper bound of prior on migration rate for admixture model',type=float,default=1e-7)
    parser.add_argument('-ne_lo','--priorNe_lo',dest='ne_lo',help='Lower bound of prior on effective population size',type=float,default=10000)
    parser.add_argument('-ne_hi','--priorNe_hi',dest='ne_hi',help='Upper bound of prior on effective population size',type=float,default=10000)
    parser.add_argument('--pGrowth',dest='pGrowth',help='Proportion of training examples that will be simulated under a model of  exponential growth',type=float,default=0.5)
    parser.add_argument('--pAdmix',dest='pAdmix',help='Proportion of training examples that will be simulated under admixture',type=float,default=0.5)
    parser.add_argument('--expansion',help='simulate under population size expansion',default=False, action='store_true')
    parser.add_argument('--admixture',help='simulate under admixture',default=False, action='store_true')
    parser.add_argument('--testGrid',dest='testGrid',help='generate test examples using meshgrid over priors',type=int,default=0)
    parser.add_argument('--gridParams',dest='gridParams',help='two parameters (comma-separated) to use for the test grid (gr,mu,ne,rho)',type=str,default=None)
    parser.add_argument('--cLen',dest='cLen',help='Length of chromosome to simulate',type=int,default=1000000)
    parser.add_argument('--sam',dest='sam',help='sample size',type=int,default=10)
    parser.add_argument('-s','--seed',dest='seed',help='Random seed',type=int,default=None)
    parser.add_argument("-t","--nCPU",dest="nCPU",help="number of cores to use (default uses all available)",
            type=int,default=None)
    parser.add_argument('--pretrim',dest='pretrim',
        help='If set, genotype and position matrixes will first be trimmed down to this length to save storage space',
        type=int,default=0)
    parser.add_argument('--trim',help='If set, genotype and position matrixes will be trimmed down to the length of the fewest number of SNPs across all simulations',default=False, action='store_true')
    parser.add_argument('--phased',help='Treat genotypes as phased',default=False, action='store_true')
    parser.add_argument('--unphased',dest='phased',help='Treat genotypes as unphased',action='store_false')
    parser.add_argument('--trial_only',dest='trial_only',help='Only simulate under "trial" parameters',default=False,action='store_true')
    parser.add_argument('--ne_no_growth',dest='ne_no_growth',
            help='Do not run trial and instead use this effective population size for no-growth sims',type=float,default=None)
    parser.add_argument('--paramsID',help='paramsID',type=int,default=None)
    parser.add_argument('--nTrial',dest='nTrial',help='Number of trial examples to simulate',type=int,default=None)
    parser.add_argument('--nTrain',dest='nTrain',help='Number of training examples to simulate',type=int,default=100000)
    parser.add_argument('--nVali',dest='nVali',help='Number of validation examples to simulate',type=int,default=1000)
    parser.add_argument('--nTest',dest='nTest',help='Number of test examples to simulate',type=int,default=1000)
    args = parser.parse_args()
    
    ## Ensure all required arguments are provided
    if not args.outDir:
        print("Warning: No project directory found, using current working directory.")
        projectDir = os.getcwd()
    else:
        projectDir = args.outDir
    
    ## Set seed
    if args.seed:
        os.environ['PYTHONHASHSEED']=str(args.seed)
        random.seed(args.seed)
        np.random.seed(args.seed)

    # set number of cores to use
    if args.nCPU:
        nProc = args.nCPU
    else:
        nProc=mp.cpu_count()
    
    ## Set up the directory structure to store the simulations data.
    if args.paramsID:
        ID = args.paramsID
        trialDir = os.path.join(projectDir,"trial_params%s"%(ID))
        trainDir = os.path.join(projectDir,"train_params%s"%(ID))
        valiDir = os.path.join(projectDir,"vali_params%s"%(ID))
        testDir = os.path.join(projectDir,"test_params%s"%(ID))
        networkDir = os.path.join(projectDir,"networks_params%s"%(ID))
    else:
        trialDir = os.path.join(projectDir,"trial")
        trainDir = os.path.join(projectDir,"train")
        valiDir = os.path.join(projectDir,"vali")
        testDir = os.path.join(projectDir,"test")
        networkDir = os.path.join(projectDir,"networks")


    ## Make directories if they do not exist
    if args.trial_only:
        if not os.path.exists(trialDir):
            os.makedirs(trialDir)
    else:
        for p in [projectDir,trialDir,trainDir,valiDir,testDir,networkDir]:
            if not os.path.exists(p):
                os.makedirs(p)
    assumedMu_lo = args.mu_lo
    assumedMu_hi = args.mu_hi
    assumedR_lo = args.r_lo
    assumedR_hi = args.r_hi
    assumedGr_lo = args.gr_lo
    assumedGr_hi = args.gr_hi
    assumedM_lo = args.m_lo
    assumedM_hi = args.m_hi
    assumedNe_lo = args.ne_lo
    assumedNe_hi = args.ne_hi
    nSamps = args.sam
    cLen = args.cLen
    
    ## Run "trial" simulations to obtain Ne under no-growth/no-admixture
    if not args.ne_no_growth:
        dg_params = {'N':nSamps,
            'Ne_growth_lo':assumedNe_lo,
            'Ne_growth_hi':assumedNe_hi,
            'priorLowsRho':assumedR_lo,
            'priorHighsRho':assumedR_hi,
            'priorLowsMu':assumedMu_lo,
            'priorHighsMu':assumedMu_hi,
            'priorLowsGr':assumedGr_lo,
            'priorHighsGr':assumedGr_hi,
            'priorLowsM':assumedM_lo,
            'priorHighsM':assumedM_hi,
            'ChromosomeLength':cLen,
            'winMasks':None,
            'maskThresh':None,
            'phased':args.phased,
            'fractionGrowth': 1.0,
            'fractionAdmix': 1.0,
            'expansion': args.expansion,
            'admixture': args.admixture,
            'seed':args.seed,
            'pretrim':args.pretrim
                  }

        # Assign pars for each simulation
        dg_trial = Simulator(**dg_params)
        
        print("\nDetermine relative Ne for simulations with and without expansion/admixture...")
        if args.nTrial:
            dg_trial.simulateAndProduceTrees(numReps=args.nTrial,direc=trialDir,simulator="msprime",nProc=nProc)
        else:
            dg_trial.simulateAndProduceTrees(numReps=int(args.nTrain/100.0),direc=trialDir,simulator="msprime",nProc=nProc)
        
        DsInfoDir = pickle.load(open(os.path.join(trialDir,"info.p"),"rb"))
        S = DsInfoDir["segSites"]
        a=0
        for i in range(nSamps-1):
            a+=1/(i+1)
        thetaW=np.average(S)/float(a)
        ne_from_theta=int(thetaW/(4.0 * np.mean([assumedMu_lo,assumedMu_hi]) * cLen))
        DsInfoDir["ne_from_theta"] = ne_from_theta
        pickle.dump(DsInfoDir,open(os.path.join(trialDir,"info.p"),"wb"))

        print("Ne inferred from thetaW:",ne_from_theta)
        if args.trial_only:
            sys.exit()
    else:
        ne_from_theta = args.ne_no_growth
    
    
    dg_params = {'N':nSamps,
        'Ne_growth_lo':assumedNe_lo,
        'Ne_growth_hi':assumedNe_hi,
        'Ne_noGrowth':ne_from_theta,
        'priorLowsRho':assumedR_lo,
        'priorHighsRho':assumedR_hi,
        'priorLowsMu':assumedMu_lo,
        'priorHighsMu':assumedMu_hi,
        'priorLowsGr':assumedGr_lo,
        'priorHighsGr':assumedGr_hi,
        'priorLowsM':assumedM_lo,
        'priorHighsM':assumedM_hi,
        'ChromosomeLength':cLen,
        'winMasks':None,
        'maskThresh':None,
        'phased':args.phased,
        'fractionGrowth': args.pGrowth,
        'fractionAdmix': args.pAdmix,
        'expansion': args.expansion,
        'admixture': args.admixture,
        'seed':args.seed,
        'pretrim':args.pretrim
              }
    
    
    ## Dump simulation pars for use with parametric bootstrap
    simParsFILE=os.path.join(networkDir,"simPars.p")
    with open(simParsFILE, "wb") as fOUT:
        if os.path.basename(projectDir):
            dg_params["bn"] = os.path.basename(projectDir)
        else:
            dg_params["bn"] = projectDir.split("/")[-2]
        pickle.dump(dg_params,fOUT)
    
    # Assign pars for each simulation
    dg_train = Simulator(**dg_params)
    dg_vali = Simulator(**dg_params)
    dg_params["testGrid"] = args.testGrid
    dg_params["gridParams"] = args.gridParams
    dg_test = Simulator(**dg_params)


    ## Simulate data
    print("\nTraining set:")
    if args.paramsID:
        dg_train.simulateAndProduceTrees(numReps=1,direc=trainDir,simulator="msprime",nProc=nProc)
        print("Validation set:")
        dg_vali.simulateAndProduceTrees(numReps=1,direc=valiDir,simulator="msprime",nProc=nProc)
    else:
        dg_train.simulateAndProduceTrees(numReps=args.nTrain,direc=trainDir,simulator="msprime",nProc=nProc)
        print("Validation set:")
        dg_vali.simulateAndProduceTrees(numReps=args.nVali,direc=valiDir,simulator="msprime",nProc=nProc)
    print("Test set:")
    dg_test.simulateAndProduceTrees(numReps=args.nTest,direc=testDir,simulator="msprime",nProc=nProc)
    print("\nSIMULATIONS FINISHED!\n")

    ## Count number of segregating sites in simulation
    SS=[]
    maxSegSites = 0
    minSegSites = float("inf")
    #for ds in [trainDir,valiDir,testDir,testDirLo,testDirHi]:
    #for ds in [trainDir,valiDir,testDir]:
    for ds in [trainDir,valiDir]:
        DsInfoDir = pickle.load(open(os.path.join(ds,"info.p"),"rb"))
        SS.extend(DsInfoDir["segSites"])
        segSitesInDs = max(DsInfoDir["segSites"])
        segSitesInDsMin = min(DsInfoDir["segSites"])
        maxSegSites = max(maxSegSites,segSitesInDs)
        minSegSites = min(minSegSites,segSitesInDsMin)
    ## Compare counts of segregating sites between simulations and input VCF
    print("SANITY CHECK")
    print("====================")
    print("numSegSites\t\t\tMin\tMean\tMax")
    print("Simulated:\t\t\t%s\t%s\t%s" %(minSegSites, int(sum(SS)/float(len(SS))), maxSegSites))
    
    ### Plot the SFS of the test set
    #outfile = os.path.join(networkDir, "testSet_sfs.pdf") 
    #sfs_const, sfs_expan = [], []
    #pi_const, pi_expan = [], []
    #info = pickle.load(open(os.path.join(testDir,"info.p"),"rb"))
    #gr = info["gr"]
    #print("Enumerating over the test set...")
    #for i, rate in enumerate(gr):
    #    progress_bar((i+1)/gr.shape[0])
    #    Hfilepath = os.path.join(testDir,str(i) + "_haps.npy")
    #    Pfilepath = os.path.join(testDir,str(i) + "_pos.npy")
    #    H = np.load(Hfilepath)
    #    P = np.load(Pfilepath)
    #    ac = allel.HaplotypeArray(H).count_alleles()
    #    pi = allel.sequence_diversity(P,ac)
    #    rawSFS = allel.sfs(ac[:, 1], n=H.shape[1])[1:]
    #    relSFS = np.divide(rawSFS,sum(rawSFS))
    #    if rate > 0.0:
    #        sfs_expan.append(relSFS)
    #        pi_expan.append(pi)
    #    else:
    #        sfs_const.append(relSFS)
    #        pi_const.append(pi)
    #sfs_const_avg = np.mean(np.array(sfs_const),axis=0)
    #sfs_expan_avg = np.mean(np.array(sfs_expan),axis=0)
    #s = [sfs_const_avg, sfs_expan_avg]
    #bins = [n + 1 for n in range(len(s[0]))]
    #vals = []
    #for i in range(len(s)):
    #    vals.append([x for x in s[i]])
    #f, ax = plt.subplots(1, 2, sharey=True, tight_layout=True, figsize=(8, 3))
    #ax[0].bar(bins, vals[0])
    #ax[1].bar(bins, vals[1])
    #if args.admixture and not args.expansion:
    #    ax[0].set_title("No admixture (pi = %s)" %(round(np.mean(pi_const),5)))
    #    ax[1].set_title("Admixture (pi = %s)" %(round(np.mean(pi_expan),5)))
    #else:
    #    ax[0].set_title("Constant size (pi = %s)" %(round(np.mean(pi_const),5)))
    #    ax[1].set_title("Exponential growth (pi = %s)" %(round(np.mean(pi_expan),5)))
    #ax[0].set_ylabel("Counts (percent)")
    #ax[0].set_xlabel("Derived allele frequency")
    #ax[1].set_xlabel("Derived allele frequency")
    #f.savefig(outfile, bbox_inches='tight')
    #plt.close()

    ## remove directories and trim
    if args.paramsID:
        shutil.rmtree(trialDir)
        shutil.rmtree(trainDir)
        shutil.rmtree(valiDir)
        trainDir = os.path.join(projectDir,"train")
        valiDir = os.path.join(projectDir,"vali")
        if args.trim:
            maxSegSites = float("inf")
            for nDir in [trainDir,valiDir]:
                S_min = min(pickle.load(open(os.path.join(nDir,"info.p"),"rb"))["segSites"])
                maxSegSites = min(maxSegSites, S_min)
            for nDir in [testDir]: #only trim testDir as train and vali would have been trimmed earlier
                print("\nTrimming genotype and position .npy files in %s to %s SNPs"%(nDir,maxSegSites))
                numReps = pickle.load(open(os.path.join(nDir,"info.p"),"rb"))["numReps"]
                for i in range(numReps):
                    Hfilepath = os.path.join(nDir, str(i) + "_haps.npy")
                    Pfilepath = os.path.join(nDir, str(i) + "_pos.npy")
                    H = np.load(Hfilepath)
                    P = np.load(Pfilepath)
                    H = H[:maxSegSites]
                    P = P[:maxSegSites]
                    np.save(Hfilepath,H)
                    np.save(Pfilepath,P)
                    progress_bar((i+1)/float(numReps))

    else:
        shutil.rmtree(trialDir)
        if args.trim:
            maxSegSites = float("inf")
            for nDir in [trainDir,valiDir]:
                S_min = min(pickle.load(open(os.path.join(nDir,"info.p"),"rb"))["segSites"])
                maxSegSites = min(maxSegSites, S_min)
            for nDir in [trainDir,valiDir,testDir]:
                print("\nTrimming genotype and position .npy files in %s to %s SNPs"%(nDir,maxSegSites))
                numReps = pickle.load(open(os.path.join(nDir,"info.p"),"rb"))["numReps"]
                for i in range(numReps):
                    Hfilepath = os.path.join(nDir, str(i) + "_haps.npy")
                    Pfilepath = os.path.join(nDir, str(i) + "_pos.npy")
                    H = np.load(Hfilepath)
                    P = np.load(Pfilepath)
                    H = H[:maxSegSites]
                    P = P[:maxSegSites]
                    np.save(Hfilepath,H)
                    np.save(Pfilepath,P)
                    progress_bar((i+1)/float(numReps))

        
    print("\n\n***FINISHED***\n")


if __name__ == "__main__":
	main()
