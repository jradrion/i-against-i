#!/usr/bin/env python
from iai.imports import *
from iai.helpers import *
from iai.sequenceBatchGenerator import *
from iai.networks import *


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-d','--projectDir',dest='outDir',help='Directory for all project output. NOTE: the same projectDir must be used for all functions of i-against-i',default=None)
    parser.add_argument('-m','--maxSegSites',dest='maxSegSites',help='All tensors exceeding this length with be trimmed', type=int, default=1000)
    parser.add_argument('-a','--attackFraction',dest='attackFraction',help='Fraction of training set to be comprised of adversarial examples', type=float, default=0.0)
    parser.add_argument('-ab','--attackBatchSize',dest='attackBatchSize',help='Size of the batch for attacking the train/vailidation sets', type=int, default=1000)
    parser.add_argument('-im','--initModel',dest='model',help='Path to the initialization model',default=None)
    parser.add_argument('-iw','--initWeights',dest='weights',help='Path to the initialization weights',default=None)
    parser.add_argument('--nEpochs',dest='nEpochs',help='Maximum number of epochs to train (EarlyStopping is implemented for validation accuracy)', type=int, default=1000)
    parser.add_argument('--nValSteps',dest='nValSteps',help='Number of validation steps', type=int, default=20)
    parser.add_argument('-s','--seed',dest='seed',help='Random seed',type=int,default=None)
    parser.add_argument('-r','--rep',dest='rep',help='Training replicate number',type=int,default=None)
    parser.add_argument('-t','--nCPU',dest='nCPU',help='Number of CPUs to use',type=int,default=1)
    parser.add_argument('--gpuID',dest='gpuID',help='Identifier specifying which GPU to use', type=int, default=0)
    args = parser.parse_args()
 

    ### Set seed
    if args.seed:
        os.environ['PYTHONHASHSEED']=str(args.seed)
        random.seed(args.seed)
        np.random.seed(args.seed)
        tf.random.set_seed(args.seed)
        os.environ['TF_DETERMINISTIC_OPS'] = '1'
        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'


    ## Set up the directory structure to store the simulations data.
    if not args.outDir:
        print("Warning: No project directory found, using current working directory.")
        projectDir = os.getcwd()
    else:
        projectDir = args.outDir
    
    
    ## Define and create the new training rep directories
    trainDir_fgsm = os.path.join(projectDir,"train_fgsm")
    valiDir_fgsm = os.path.join(projectDir,"vali_fgsm")
    #trainDir_pgd = os.path.join(projectDir,"train_pgd")
    #valiDir_pgd = os.path.join(projectDir,"vali_pgd")
    for p in [trainDir_fgsm,valiDir_fgsm]:
    #for p in [trainDir_fgsm,trainDir_pgd,valiDir_fgsm,valiDir_pgd]:
        if args.rep:
            p += "_rep%s"%(args.rep)
        if not os.path.exists(p):
            os.makedirs(p)
    
    if args.rep:
        networkDir_OG = os.path.join(projectDir,"networks")
        networkDir = os.path.join(projectDir,"networks"+"_rep%s"%(args.rep)) 
        cmd="cp -r %s %s"%(networkDir_OG, networkDir)
        os.system(cmd)
    else:
        networkDir = os.path.join(projectDir,"networks")
    
    trainDir = os.path.join(projectDir,"train")
    valiDir = os.path.join(projectDir,"vali")
    testDir = os.path.join(projectDir,"test")
    
    
    ## Define output files
    test_resultFile = os.path.join(networkDir,"testResults.p")
    test_resultFig = os.path.join(networkDir,"testResults.pdf")
    modelSave = os.path.join(networkDir,"model.json")
    weightsSave = os.path.join(networkDir,"weights.h5")


    ## Identify padding required
    ## using maxSegSites=0 will trim all examples to the length of the example with the fewest sites
    maxSegSites = float("inf")
    if args.maxSegSites == 0:
        for nDir in [trainDir,valiDir]:
            S_min = min(pickle.load(open(os.path.join(nDir,"info.p"),"rb"))["segSites"])
            maxSegSites = min(maxSegSites, S_min)
    else:
        maxSegSites = args.maxSegSites

    
    bds_train_params = {
        'treesDirectory':trainDir,
        'targetNormalization':"zscore",
        'batchSize': 64,
        'maxLen': maxSegSites,
        'frameWidth': 0,
        'shuffleInds':True,
        'sortInds':False,
        'center':False,
        'ancVal':0,
        'padVal':0,
        'derVal':1,
        'realLinePos':True,
        'posPadVal':0,
        'seqD':None,
        'rep':args.rep
              }


    ## Dump batch pars for bootstrap
    batchParsFILE=os.path.join(networkDir,"batchPars.p")
    with open(batchParsFILE, "wb") as fOUT:
        pickle.dump(bds_train_params,fOUT)


    bds_vali_params = copy.deepcopy(bds_train_params)
    bds_vali_params['treesDirectory'] = valiDir
    bds_vali_params['batchSize'] = 64

    bds_test_params = copy.deepcopy(bds_train_params)
    bds_test_params['treesDirectory'] = testDir
    DsInfoDir = pickle.load(open(os.path.join(testDir,"info.p"),"rb"))
    bds_test_params['batchSize'] = DsInfoDir["numReps"]
    bds_test_params['shuffleExamples'] = False


    ## Define sequence batch generator
    train_sequence = SequenceBatchGenerator(**bds_train_params)
    vali_sequence = SequenceBatchGenerator(**bds_vali_params)
    test_sequence = SequenceBatchGenerator(**bds_test_params)

    
    # Train network
    runModels_cleverhans_tf2(ModelFuncPointer=iaiGRU_categorical_crossentropy_noPos,
            ModelName="iaiGRU_categorical_crossentropy_noPos",
            NetworkDir=networkDir,
            ProjectDir=projectDir,
            TrainGenerator=train_sequence,
            TrainParams=bds_train_params,
            ValiParams=bds_vali_params,
            ValidationGenerator=vali_sequence,
            TestGenerator=test_sequence,
            resultsFile=test_resultFile,
            initModel=args.model,
            initWeights=args.weights,
            network=[modelSave,weightsSave],
            numEpochs=args.nEpochs,
            validationSteps=args.nValSteps,
            nCPU=args.nCPU,
            gpuID=args.gpuID,
            learningRate=0.001,
            attackFraction=args.attackFraction,
            attackBatchSize=args.attackBatchSize,
            rep=args.rep) 
    
    ### Plot results of predictions on test set
    plotResultsSoftmax2Heatmap(resultsFile=test_resultFile,saveas=test_resultFig)
    plotResultsSoftmax2Heatmap(resultsFile=test_resultFile.replace(".p","_fgsm.p"),saveas=test_resultFig.replace(".pdf","_fgsm.pdf"))
    ##plotResultsSoftmax2Heatmap(resultsFile=test_resultFile.replace(".p","_pgd.p"),saveas=test_resultFig.replace(".pdf","_pgd.pdf"))


    print("\n***FINISHED!***\n")

if __name__ == "__main__":
	main()
